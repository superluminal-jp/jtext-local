"""
Context-aware LLM correction for improved accuracy.

This module provides advanced correction capabilities that use context
information to improve the quality of OCR and ASR corrections.
"""

import time
from typing import Dict, Any, List, Optional, Tuple
import json
import requests

from ..utils.logging import get_logger


logger = get_logger(__name__)


class ContextAwareCorrector:
    """
    Context-aware LLM correction for improved accuracy.

    This class provides advanced correction capabilities that use context
    information to improve the quality of OCR and ASR corrections.
    """

    def __init__(self, model: str = "llama2", context_window: int = 2048):
        """
        Initialize the context-aware corrector.

        Args:
            model: LLM model name to use for correction
            context_window: Maximum context window size
        """
        self.model = model
        self.context_window = context_window
        self._model_available = self._check_model_availability()

        logger.info(f"Initialized ContextAwareCorrector with model: {model}")

    def correct_with_context(
        self,
        text: str,
        context_type: str = "general",
        document_metadata: Optional[Dict[str, Any]] = None,
        previous_text: Optional[str] = None,
    ) -> Tuple[str, int]:
        """
        Correct text using context-aware LLM prompts.

        Args:
            text: Text to correct
            context_type: Type of context (general, academic, business, technical)
            document_metadata: Metadata about the source document
            previous_text: Previous text for context

        Returns:
            Tuple of (corrected_text, number_of_corrections)
        """
        if not text.strip():
            return text, 0

        logger.debug(f"Correcting text with context: {context_type}")

        if self._model_available:
            corrected_text = self._context_aware_llm_correct(
                text, context_type, document_metadata, previous_text
            )
        else:
            corrected_text = self._rule_based_correct(text)

        # Count corrections made
        corrections = self._count_corrections(text, corrected_text)

        logger.debug(f"Applied {corrections} corrections with context")
        return corrected_text, corrections

    def _context_aware_llm_correct(
        self,
        text: str,
        context_type: str,
        document_metadata: Optional[Dict[str, Any]],
        previous_text: Optional[str],
    ) -> str:
        """
        Correct text using context-aware LLM prompts.

        Args:
            text: Text to correct
            context_type: Type of context
            document_metadata: Document metadata
            previous_text: Previous text for context

        Returns:
            Context-aware corrected text
        """
        try:
            # Build context-aware prompt
            prompt = self._build_context_prompt(
                text, context_type, document_metadata, previous_text
            )

            # Prepare request payload
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "max_tokens": self.context_window,
                },
            }

            logger.debug(f"Calling context-aware LLM with model: {self.model}")

            # Make request to Ollama API
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=60,  # Longer timeout for context-aware processing
            )

            if response.status_code == 200:
                result = response.json()
                corrected_text = result.get("response", "").strip()

                # Clean up the response
                corrected_text = self._clean_response(corrected_text, prompt)

                logger.info(
                    f"Context-aware correction completed: {len(corrected_text)} characters"
                )
                return corrected_text
            else:
                logger.error(f"LLM API error: {response.status_code} - {response.text}")
                return self._rule_based_correct(text)

        except Exception as e:
            logger.error(f"Context-aware correction failed: {e}")
            return self._rule_based_correct(text)

    def _build_context_prompt(
        self,
        text: str,
        context_type: str,
        document_metadata: Optional[Dict[str, Any]],
        previous_text: Optional[str],
    ) -> str:
        """
        Build context-aware prompt for LLM correction.

        Args:
            text: Text to correct
            context_type: Type of context
            document_metadata: Document metadata
            previous_text: Previous text for context

        Returns:
            Context-aware prompt
        """
        # Base correction instructions
        base_instructions = """ã‚ãªãŸã¯é«˜åº¦ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèªè­˜æ©Ÿèƒ½ã‚’æŒã¤ãƒ†ã‚­ã‚¹ãƒˆä¿®æ­£å°‚é–€å®¶ã§ã™ã€‚OCRã¾ãŸã¯éŸ³å£°èªè­˜ã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã€æ–‡è„ˆãƒ»æ–‡æ›¸ã‚¿ã‚¤ãƒ—ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç·åˆçš„ã«åˆ†æžã—ã€æœ€é«˜å“è³ªã®æ—¥æœ¬èªžãƒ†ã‚­ã‚¹ãƒˆã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèªè­˜ä¿®æ­£ã®åŽŸå‰‡

## 1. å¤šå±¤çš„æ–‡è„ˆåˆ†æž
### ãƒžã‚¯ãƒ­æ–‡è„ˆï¼ˆæ–‡æ›¸ãƒ¬ãƒ™ãƒ«ï¼‰:
- æ–‡æ›¸ã®ç¨®é¡žã¨ç›®çš„ã®ç†è§£
- æƒ³å®šèª­è€…å±¤ã¨å°‚é–€æ€§ãƒ¬ãƒ™ãƒ«
- æ–‡æ›¸å…¨ä½“ã®è«–ç†æ§‹é€ ã¨æµã‚Œ

### ãƒŸã‚¯ãƒ­æ–‡è„ˆï¼ˆæ–‡ãƒ»æ®µè½ãƒ¬ãƒ™ãƒ«ï¼‰:
- å‰å¾Œã®æ–‡ã¨ã®é–¢ä¿‚æ€§
- æ®µè½å†…ã®è«–ç†çš„ä¸€è²«æ€§
- èªžå½™é¸æŠžã®é©åˆ‡æ€§

### ãƒ¡ã‚¿æ–‡è„ˆï¼ˆå¤–éƒ¨æƒ…å ±ï¼‰:
- åˆ†é‡Žç‰¹æœ‰ã®å°‚é–€ç”¨èªžãƒ»æ…£ä¾‹
- æ™‚ä»£èƒŒæ™¯ãƒ»åœ°åŸŸæ€§
- æ–‡æ›¸ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã®æ¨™æº–

## 2. éšŽå±¤çš„ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
### Level 1: åŸºæœ¬çš„ã‚¨ãƒ©ãƒ¼ä¿®æ­£
- æ–‡å­—èªè­˜ãƒŸã‚¹ã®è¨‚æ­£
- åŸºæœ¬çš„ãªæ–‡æ³•ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
- æ˜Žã‚‰ã‹ãªèª¤å­—è„±å­—ã®è¨‚æ­£

### Level 2: æ–‡è„ˆæ•´åˆæ€§ã®ç¢ºä¿
- æ–‡ç« ã®è«–ç†çš„ä¸€è²«æ€§ã®æ¤œè¨¼
- å°‚é–€ç”¨èªžã®çµ±ä¸€ã¨æ­£ç¢ºæ€§
- æ–‡ä½“ãƒ»æ•¬èªžãƒ¬ãƒ™ãƒ«ã®çµ±ä¸€

### Level 3: å“è³ªæœ€é©åŒ–
- èª­ã¿ã‚„ã™ã•ã®å‘ä¸Š
- å†—é•·æ€§ã®é™¤åŽ»
- è¡¨ç¾ã®è‡ªç„¶æ€§å‘ä¸Š

## 3. æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¥æœ€é©åŒ–
### æŠ€è¡“æ–‡æ›¸:
- æ­£ç¢ºæ€§ã¨æ˜Žç¢ºæ€§ã‚’æœ€å„ªå…ˆ
- å°‚é–€ç”¨èªžã®ä¸€è²«ã—ãŸä½¿ç”¨
- æ‰‹é †ãƒ»ä»•æ§˜ã®è«–ç†çš„é…åˆ—

### å­¦è¡“æ–‡æ›¸:
- å®¢è¦³æ€§ã¨å­¦è¡“çš„è¡¨ç¾ã®ç¶­æŒ
- å¼•ç”¨ãƒ»å‚è€ƒæ–‡çŒ®ã®é©åˆ‡ãªå‡¦ç†
- è«–ç†çš„è«–è¨¼ã®æ§‹é€ ä¿æŒ

### ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸:
- ä¸å¯§èªžãƒ»æ•¬èªžã®é©åˆ‡ãªä½¿ç”¨
- ç°¡æ½”ã§åŠ¹æžœçš„ãªè¡¨ç¾
- ç›®çš„é”æˆã¸ã®ç„¦ç‚¹åŒ–

### ä¸€èˆ¬æ–‡æ›¸:
- èª­ã¿ã‚„ã™ã•ã¨è¦ªã—ã¿ã‚„ã™ã•
- å¤šæ§˜ãªèª­è€…å±¤ã¸ã®é…æ…®
- è‡ªç„¶ãªæ—¥æœ¬èªžè¡¨ç¾

# å“è³ªåŸºæº–

## å¿…é ˆé”æˆé …ç›®:
âœ… æ–‡è„ˆã«å®Œå…¨ã«é©åˆã—ãŸä¿®æ­£
âœ… æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé©åˆ‡ãªæ–‡ä½“
âœ… å°‚é–€ç”¨èªžãƒ»å›ºæœ‰åè©žã®æ­£ç¢ºæ€§
âœ… è«–ç†çš„ä¸€è²«æ€§ã®ä¿æŒ
âœ… è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªž

## ç¦æ­¢è¡Œç‚º:
âŒ æ–‡è„ˆã‚’ç„¡è¦–ã—ãŸæ©Ÿæ¢°çš„ä¿®æ­£
âŒ å…ƒã®æ„å‘³ãƒ»æƒ…å ±ã®æ”¹å¤‰
âŒ ä¸é©åˆ‡ãªæ–‡ä½“ã®æ··åœ¨
âŒ æŽ¨æ¸¬ã«ã‚ˆã‚‹æƒ…å ±è¿½åŠ 
âŒ æ§‹é€ çš„æ•´åˆæ€§ã®ç ´æ

ä¿®æ­£ä½œæ¥­ã®åŸºæœ¬æ–¹é‡ã‚’ç†è§£ã—ã¾ã—ãŸã€‚ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åŸºã«ã€æœ€é©ãªä¿®æ­£ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"""

        # Add context-specific instructions
        context_instructions = self._get_context_instructions(context_type)

        # Add document metadata context
        metadata_context = ""
        if document_metadata:
            metadata_context = self._format_metadata_context(document_metadata)

        # Add previous text context
        previous_context = ""
        if previous_text:
            previous_context = f"\nå‰ã®æ–‡è„ˆ:\n{previous_text[:500]}...\n"

        # Build final prompt
        prompt = f"""{base_instructions}

{context_instructions}

{metadata_context}

{previous_context}

å…ƒãƒ†ã‚­ã‚¹ãƒˆï¼š
{text}

ä¿®æ­£ç‰ˆï¼š"""

        return prompt

    def _get_context_instructions(self, context_type: str) -> str:
        """
        Get context-specific correction instructions.

        Args:
            context_type: Type of context

        Returns:
            Context-specific instructions
        """
        instructions = {
            "academic": """
# ðŸŽ“ å­¦è¡“æ–‡æ›¸å°‚ç”¨ä¿®æ­£æˆ¦ç•¥

## è¨€èªžå“è³ªåŸºæº–:
- **å°‚é–€ç”¨èªž**: å­¦è¡“åˆ†é‡Žå›ºæœ‰ã®ç”¨èªžã‚’æ­£ç¢ºã«ä½¿ç”¨
- **æ–‡ä½“**: å®¢è¦³çš„ã§è«–ç†çš„ãªã€Œã§ã‚ã‚‹èª¿ã€ã‚’ç¶­æŒ
- **æ§‹é€ **: è«–ç†çš„è«–è¨¼ã®æµã‚Œã‚’é‡è¦–

## ç‰¹åˆ¥æ³¨æ„äº‹é …:
- å¼•ç”¨å½¢å¼ï¼ˆAPAã€MLAç­‰ï¼‰ã®æ­£ç¢ºãªä¿æŒ
- æ•°å€¤ãƒ»çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ç²¾å¯†æ€§ç¢ºä¿
- ä»®èª¬ãƒ»çµè«–ã®è«–ç†çš„æ•´åˆæ€§ç¶­æŒ
- å‚è€ƒæ–‡çŒ®ãƒªã‚¹ãƒˆã®å®Œå…¨æ€§

## å°‚é–€æ€§ã®ä¿æŒ:
- å­¦è¡“ç”¨èªžã®ä¸€èˆ¬åŒ–ã‚’é¿ã‘ã‚‹
- å°‚é–€æ¦‚å¿µã®æ­£ç¢ºãªè¡¨ç¾
- ç ”ç©¶æ‰‹æ³•ã®é©åˆ‡ãªè¨˜è¿°
""",
            "business": """
# ðŸ’¼ ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸å°‚ç”¨ä¿®æ­£æˆ¦ç•¥

## æ•¬èªžãƒ»æ–‡ä½“ç®¡ç†:
- **ä¸å¯§èªž**: ç¤¾å†…å¤–ã«é©ã—ãŸæ•¬èªžãƒ¬ãƒ™ãƒ«
- **ç°¡æ½”æ€§**: è¦ç‚¹ã‚’æ˜Žç¢ºã«ä¼ãˆã‚‹è¡¨ç¾
- **ä¸€è²«æ€§**: æ–‡æ›¸å…¨ä½“ã®æ•¬èªžãƒ¬ãƒ™ãƒ«çµ±ä¸€

## ãƒ“ã‚¸ãƒã‚¹æ¨™æº–:
- ä¼æ¥­åãƒ»å½¹è·åã®æ­£ç¢ºãªè¡¨è¨˜
- æ—¥ä»˜ãƒ»æ™‚åˆ»ãƒ»æ•°å€¤ã®æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ
- å¥‘ç´„ãƒ»ææ¡ˆæ›¸ã®æ³•çš„è¡¨ç¾ã®ç¶­æŒ
- ä¼šè­°éŒ²ãƒ»å ±å‘Šæ›¸ã®æ§‹é€ çš„è¨˜è¿°

## åŠ¹æžœçš„ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³:
- èª­ã¿æ‰‹ã®ç«‹å ´ã‚’è€ƒæ…®ã—ãŸè¡¨ç¾
- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®æ˜Žç¢ºåŒ–
- çµè«–ãƒ»ææ¡ˆã®å‰é¢é…ç½®
""",
            "technical": """
# ðŸ”§ æŠ€è¡“æ–‡æ›¸å°‚ç”¨ä¿®æ­£æˆ¦ç•¥

## æŠ€è¡“ç²¾åº¦ã®ç¢ºä¿:
- **å°‚é–€ç”¨èªž**: æŠ€è¡“åˆ†é‡Žå›ºæœ‰ã®æ­£ç¢ºãªç”¨èªžä½¿ç”¨
- **ä»•æ§˜è¨˜è¿°**: æŠ€è¡“ä»•æ§˜ã®åŽ³å¯†ãªè¡¨ç¾
- **æ‰‹é †èª¬æ˜Ž**: å®Ÿè¡Œå¯èƒ½ãªæ˜Žç¢ºãªæ‰‹é †è¨˜è¿°

## ã‚³ãƒ¼ãƒ‰ãƒ»ã‚³ãƒžãƒ³ãƒ‰ä¿è­·:
- ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚³ãƒ¼ãƒ‰ã®æ–‡æ³•çš„æ­£ç¢ºæ€§
- ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒžãƒ³ãƒ‰ã®å®Ÿè¡Œå¯èƒ½æ€§
- APIä»•æ§˜ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡æ³•éµå®ˆ

## æŠ€è¡“æ–‡æ›¸ç‰¹æœ‰ã®æ§‹é€ :
- å‰ææ¡ä»¶ãƒ»ç’°å¢ƒè¦ä»¶ã®æ˜Žè¨˜
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ãƒ»äº’æ›æ€§ã®è¨˜è¿°
- æŠ€è¡“çš„åˆ¶ç´„ãƒ»æ³¨æ„äº‹é …ã®å¼·èª¿
""",
            "general": """
# ðŸ“ ä¸€èˆ¬æ–‡æ›¸ä¿®æ­£æˆ¦ç•¥

## è‡ªç„¶ãªæ—¥æœ¬èªžã¸ã®æœ€é©åŒ–:
- **èª­ã¿ã‚„ã™ã•**: å¹³æ˜“ã§ç†è§£ã—ã‚„ã™ã„è¡¨ç¾
- **æµã‚Œ**: è‡ªç„¶ãªæ–‡ç« ã®æµã‚Œã¨æŽ¥ç¶š
- **èªžå½™**: é©åˆ‡ãªèªžå½™ãƒ¬ãƒ™ãƒ«ã®é¸æŠž

## å¤šæ§˜ãªèª­è€…ã¸ã®é…æ…®:
- å°‚é–€ç”¨èªžã®åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜Ž
- æ–‡è„ˆã«å¿œã˜ãŸä¾‹ç¤ºãƒ»æ¯”å–©ã®æ´»ç”¨
- æ®µè½æ§‹æˆã®è«–ç†æ€§
- çµè«–ãƒ»è¦ç‚¹ã®æ˜Žç¢ºåŒ–

## è¡¨ç¾ã®è±Šã‹ã•:
- å˜èª¿ãªè¡¨ç¾ã®å›žé¿
- é©åˆ‡ãªä¿®è¾žæŠ€æ³•ã®ä½¿ç”¨
- æ„Ÿæƒ…çš„é…æ…®ã®ã‚ã‚‹è¡¨ç¾
- æ–‡åŒ–çš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è€ƒæ…®
""",
            "vision_enhanced": """
# ðŸ‘ï¸ Visionå¼·åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿®æ­£æˆ¦ç•¥

## ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æƒ…å ±çµ±åˆ:
- **ç”»åƒåˆ†æžçµæžœ**: Visionåˆ†æžã®å†…å®¹ã‚’æœ€å¤§é™æ´»ç”¨
- **æ§‹é€ æ•´åˆæ€§**: ç”»åƒãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¨ãƒ†ã‚­ã‚¹ãƒˆæ§‹é€ ã®å®Œå…¨ä¸€è‡´
- **è¦–è¦šçš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ**: ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æ–‡è„ˆæƒ…å ±ã®æ´»ç”¨

## é«˜ç²¾åº¦ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
- OCRã‚¨ãƒ©ãƒ¼ã®ç”»åƒæƒ…å ±ã«ã‚ˆã‚‹ä¿®æ­£
- æ–‡æ›¸ã‚¿ã‚¤ãƒ—ç‰¹å®šæƒ…å ±ã®æ´»ç”¨
- ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ§‹é€ ã®æ­£ç¢ºãªå†ç¾
- å°‚é–€åˆ†é‡Žåˆ¤å®šã«ã‚ˆã‚‹ç”¨èªžä¿®æ­£

## Visionåˆ†æžå“è³ªã®æ´»ç”¨:
- æ–‡æ›¸ã®ç›®çš„ãƒ»èª­è€…å±¤ã®æŽ¨å®šæ´»ç”¨
- è¡¨ãƒ»å›³è¡¨æ§‹é€ ã®è©³ç´°å†ç¾
- æ–‡å­—å“è³ªæƒ…å ±ã«ã‚ˆã‚‹ä¿®æ­£å„ªå…ˆåº¦æ±ºå®š
- æ–‡æ›¸å®Œå…¨æ€§ã®ç¢ºä¿
""",
        }

        return instructions.get(context_type, instructions["general"])

    def _format_metadata_context(self, metadata: Dict[str, Any]) -> str:
        """
        Format document metadata for context.

        Args:
            metadata: Document metadata

        Returns:
            Formatted metadata context
        """
        context_parts = []

        if "format" in metadata:
            context_parts.append(f"æ–‡æ›¸å½¢å¼: {metadata['format']}")

        if "pages" in metadata:
            context_parts.append(f"ãƒšãƒ¼ã‚¸æ•°: {metadata['pages']}")

        if "has_tables" in metadata and metadata["has_tables"]:
            context_parts.append("è¡¨ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")

        if "has_images" in metadata and metadata["has_images"]:
            context_parts.append("ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã¾ã™")

        if "language" in metadata:
            context_parts.append(f"è¨€èªž: {metadata['language']}")

        if context_parts:
            return f"æ–‡æ›¸æƒ…å ±:\n" + "\n".join(context_parts) + "\n"

        return ""

    def _clean_response(self, response: str, prompt: str) -> str:
        """
        Clean up LLM response.

        Args:
            response: Raw LLM response
            prompt: Original prompt

        Returns:
            Cleaned response
        """
        # Remove prompt if it was included in response
        if response.startswith(prompt):
            response = response[len(prompt) :].strip()

        # Remove common prefixes
        prefixes_to_remove = [
            "ä¿®æ­£ç‰ˆï¼š",
            "ä¿®æ­£ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼š",
            "ä¿®æ­£çµæžœï¼š",
            "Corrected text:",
            "ä¿®æ­£:",
        ]

        for prefix in prefixes_to_remove:
            if response.startswith(prefix):
                response = response[len(prefix) :].strip()

        return response

    def _check_model_availability(self) -> bool:
        """
        Check if the specified LLM model is available.

        Returns:
            True if model is available, False otherwise
        """
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]

                if self.model in available_models:
                    logger.info(f"Context-aware model {self.model} is available")
                    return True
                else:
                    logger.warning(
                        f"Model {self.model} not found. Available: {available_models}"
                    )
                    return False
            else:
                logger.warning("Ollama API not responding")
                return False
        except requests.exceptions.RequestException as e:
            logger.warning(f"Ollama not available: {e}")
            return False

    def _rule_based_correct(self, text: str) -> str:
        """
        Apply rule-based corrections as fallback.

        Args:
            text: Text to correct

        Returns:
            Rule-based corrected text
        """
        # Basic rule-based corrections
        corrected = text

        # Common OCR error corrections
        corrections = {
            "ï¼": "0",
            "ï¼‘": "1",
            "ï¼’": "2",
            "ï¼“": "3",
            "ï¼”": "4",
            "ï¼•": "5",
            "ï¼–": "6",
            "ï¼—": "7",
            "ï¼˜": "8",
            "ï¼™": "9",
            "ï¼Œ": "ã€",
            "ï¼Ž": "ã€‚",
            "ï¼š": "ï¼š",
            "ï¼›": "ï¼›",
            "ã€€ã€€": "ã€€",
            "  ": " ",
        }

        for wrong, correct in corrections.items():
            corrected = corrected.replace(wrong, correct)

        return corrected

    def _count_corrections(self, original: str, corrected: str) -> int:
        """
        Count the number of corrections made.

        Args:
            original: Original text
            corrected: Corrected text

        Returns:
            Number of corrections made
        """
        if len(original) != len(corrected):
            return abs(len(original) - len(corrected))

        return sum(1 for a, b in zip(original, corrected) if a != b)
