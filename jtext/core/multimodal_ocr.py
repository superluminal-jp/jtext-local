"""
Multimodal OCR processing with image understanding and text extraction.

This module implements advanced OCR functionality that combines:
1. Traditional OCR (Tesseract) for text extraction
2. Vision-Language Models for image understanding
3. LLM-based fusion for optimal results
"""

import time
import base64
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
import pytesseract
from PIL import Image
import psutil
import requests
import json

from ..utils.logging import get_logger
from ..preprocessing.image_prep import ImagePreprocessor
from ..correction.ocr_corrector import OCRCorrector
from ..correction.context_aware_corrector import ContextAwareCorrector
from ..utils.validation import validate_image_file


logger = get_logger(__name__)


class MultimodalOCRResult:
    """Container for multimodal OCR processing results with enhanced metadata."""

    def __init__(
        self,
        source_path: str,
        text: str,
        confidence: float,
        processing_time: float,
        memory_usage: float,
        corrections_applied: int = 0,
        correction_ratio: float = 0.0,
        vision_analysis: Optional[Dict[str, Any]] = None,
        fusion_method: str = "ocr_only",
    ):
        self.source_path = source_path
        self.text = text
        self.confidence = confidence
        self.processing_time = processing_time
        self.memory_usage = memory_usage
        self.corrections_applied = corrections_applied
        self.correction_ratio = correction_ratio
        self.vision_analysis = vision_analysis or {}
        self.fusion_method = fusion_method
        self.timestamp = time.time()

    def to_dict(self) -> Dict[str, Any]:
        """Convert result to dictionary for JSON serialization."""
        return {
            "source": str(Path(self.source_path).absolute()),
            "type": "image",
            "timestamp": time.strftime(
                "%Y-%m-%dT%H:%M:%SZ", time.gmtime(self.timestamp)
            ),
            "processing": {
                "pipeline": self._get_pipeline(),
                "fusion_method": self.fusion_method,
                "ocr_engine": "tesseract-5.3.3",
                "vision_model": self.vision_analysis.get("model", None),
                "llm_model": "gpt-oss" if self.corrections_applied > 0 else None,
                "confidence": {
                    "ocr_raw": self.confidence,
                    "vision_analysis": self.vision_analysis.get("confidence", None),
                    "llm_corrected": (
                        self.confidence if self.corrections_applied > 0 else None
                    ),
                },
            },
            "vision_analysis": self.vision_analysis,
            "correction_stats": {
                "characters_changed": self.corrections_applied,
                "words_changed": self.corrections_applied // 2,
                "correction_ratio": self.correction_ratio,
                "correction_types": (
                    ["kanji_fix", "punctuation", "layout", "vision_enhanced"]
                    if self.corrections_applied > 0
                    else []
                ),
            },
            "quality_metrics": {
                "character_count": len(self.text),
                "word_count": len(self.text.split()),
                "processing_time_sec": self.processing_time,
                "memory_usage_mb": self.memory_usage,
            },
        }

    def _get_pipeline(self) -> List[str]:
        """Get processing pipeline based on enabled features."""
        pipeline = ["tesseract"]
        if self.vision_analysis:
            pipeline.append("vision_analysis")
        if self.corrections_applied > 0:
            pipeline.append("llm_correction")
        return pipeline


class MultimodalOCR:
    """
    Advanced OCR processing combining traditional OCR with vision-language models.

    This class provides multimodal OCR functionality that:
    1. Extracts text using Tesseract OCR
    2. Analyzes image content using vision models
    3. Fuses results using LLM for optimal accuracy
    """

    def __init__(
        self,
        llm_model: Optional[str] = None,
        vision_model: Optional[str] = None,
        enable_correction: bool = False,
        enable_vision: bool = False,
    ):
        """
        Initialize the multimodal OCR processor.

        Args:
            llm_model: LLM model name for correction and fusion
            vision_model: Vision model name for image analysis
            enable_correction: Whether to enable LLM correction
            enable_vision: Whether to enable vision analysis
        """
        self.enable_correction = enable_correction
        self.enable_vision = enable_vision
        self.llm_model = llm_model
        self.vision_model = vision_model or "llava"
        self.preprocessor = ImagePreprocessor()
        self.corrector = OCRCorrector(model=llm_model) if enable_correction else None
        self.context_corrector = (
            ContextAwareCorrector(model=llm_model) if enable_correction else None
        )

        logger.info(
            f"Initialized MultimodalOCR - Vision: {enable_vision}, "
            f"Correction: {enable_correction}"
        )

    def process_image(self, image_path: str) -> MultimodalOCRResult:
        """
        Process image through multimodal OCR pipeline.

        Args:
            image_path: Path to the image file

        Returns:
            MultimodalOCRResult with enhanced text and metadata

        Raises:
            FileNotFoundError: If image file doesn't exist
            ValueError: If image file is invalid
        """
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

        logger.info(f"Starting multimodal OCR processing for: {image_path}")

        # Validate input file
        if not validate_image_file(image_path):
            raise ValueError(f"Invalid image file: {image_path}")

        try:
            # Load and preprocess image
            image = Image.open(image_path)
            processed_image = self.preprocessor.preprocess(image)

            # Step 1: Traditional OCR
            ocr_text, ocr_confidence = self._extract_text_ocr(processed_image)

            # Step 2: Vision analysis (if enabled)
            vision_analysis = None
            if self.enable_vision:
                vision_analysis = self._analyze_image_vision(image_path)

            # Step 3: Fusion and correction
            final_text, corrections_applied, correction_ratio, fusion_method = (
                self._fuse_and_correct(ocr_text, vision_analysis, image_path)
            )

            # Calculate processing metrics
            processing_time = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            memory_usage = end_memory - start_memory

            result = MultimodalOCRResult(
                source_path=image_path,
                text=final_text,
                confidence=ocr_confidence,
                processing_time=processing_time,
                memory_usage=memory_usage,
                corrections_applied=corrections_applied,
                correction_ratio=correction_ratio,
                vision_analysis=vision_analysis,
                fusion_method=fusion_method,
            )

            logger.info(
                f"Multimodal OCR completed in {processing_time:.2f}s, "
                f"confidence: {ocr_confidence:.2f}, corrections: {corrections_applied}, "
                f"fusion: {fusion_method}"
            )

            return result

        except Exception as e:
            logger.error(f"Multimodal OCR processing failed for {image_path}: {e}")
            raise

    def _extract_text_ocr(self, image: Image.Image) -> Tuple[str, float]:
        """Extract text using traditional OCR."""
        logger.debug("Running Tesseract OCR")
        ocr_text = pytesseract.image_to_string(
            image,
            lang="jpn+eng",
            config="--psm 6",  # Uniform block of text
        )

        # Get confidence score
        ocr_data = pytesseract.image_to_data(
            image, lang="jpn+eng", output_type=pytesseract.Output.DICT
        )
        confidence = self._calculate_confidence(ocr_data)

        return ocr_text.strip(), confidence

    def _analyze_image_vision(self, image_path: str) -> Optional[Dict[str, Any]]:
        """Analyze image using vision-language model."""
        if not self.enable_vision:
            return None

        logger.debug("Running vision analysis")
        try:
            # Encode image to base64
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")

            # Prepare vision analysis prompt
            vision_prompt = self._create_vision_prompt()

            # Call Ollama with vision model
            payload = {
                "model": self.vision_model,
                "prompt": vision_prompt,
                "images": [image_data],
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "max_tokens": 1024,
                },
            }

            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=300,
            )

            if response.status_code == 200:
                result = response.json()
                vision_text = result.get("response", "").strip()

                return {
                    "model": self.vision_model,
                    "analysis": vision_text,
                    "confidence": 0.8,  # Vision models typically have high confidence
                    "timestamp": time.time(),
                }
            else:
                logger.warning(f"Vision analysis failed: {response.status_code}")
                return None

        except Exception as e:
            logger.error(f"Vision analysis error: {e}")
            return None

    def _create_vision_prompt(self) -> str:
        """Create comprehensive vision analysis prompt following best practices."""
        return """ã‚ãªãŸã¯ç”»åƒè§£æžã®å°‚é–€å®¶ã§ã™ã€‚ã“ã®ç”»åƒã‚’ç·åˆçš„ã«åˆ†æžã—ã€OCRå‡¦ç†ã®ç²¾åº¦å‘ä¸Šã«å¿…è¦ãªæƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

# åˆ†æžæŒ‡ç¤º

## 1. æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã®è­˜åˆ¥
ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’æ˜Žç¢ºã«åˆ¤å®šã—ã¦ãã ã•ã„ï¼š
- æŠ€è¡“æ–‡æ›¸ï¼ˆä»•æ§˜æ›¸ã€ãƒžãƒ‹ãƒ¥ã‚¢ãƒ«ã€APIæ–‡æ›¸ãªã©ï¼‰
- å­¦è¡“è«–æ–‡ï¼ˆç ”ç©¶è«–æ–‡ã€å­¦ä¼šè³‡æ–™ãªã©ï¼‰
- ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸ï¼ˆå ±å‘Šæ›¸ã€ãƒ—ãƒ¬ã‚¼ãƒ³è³‡æ–™ãªã©ï¼‰
- è¡¨ãƒ»å›³è¡¨ï¼ˆãƒ‡ãƒ¼ã‚¿è¡¨ã€ã‚°ãƒ©ãƒ•ã€ãƒãƒ£ãƒ¼ãƒˆãªã©ï¼‰
- ä¸€èˆ¬æ–‡æ›¸ï¼ˆè¨˜äº‹ã€ãƒ–ãƒ­ã‚°ã€ãã®ä»–ï¼‰

## 2. ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ§‹é€ ã®è©³ç´°åˆ†æž
### æ§‹é€ è¦ç´ ã®ç‰¹å®šï¼š
- è¡¨ã®æœ‰ç„¡ã¨æ§‹é€ ï¼ˆè¡Œæ•°ã€åˆ—æ•°ã€ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰
- ãƒªã‚¹ãƒˆãƒ»ç®‡æ¡æ›¸ãã®éšŽå±¤
- æ®µè½ã®åŒºåˆ‡ã‚Šã¨æ§‹é€ 
- å›³è¡¨ãƒ»ç”»åƒã®é…ç½®
- ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ã®å­˜åœ¨

### æ–‡å­—æƒ…å ±ã®åˆ†æžï¼š
- ä¸»è¦è¨€èªžï¼ˆæ—¥æœ¬èªžã€è‹±èªžã€æ··åœ¨ï¼‰
- ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã®å¤‰åŒ–
- å¤ªå­—ãƒ»æ–œä½“ã®ä½¿ç”¨
- æ–‡å­—å¯†åº¦ã¨èª­ã¿ã‚„ã™ã•

## 3. OCRèª²é¡Œã®äºˆæ¸¬
ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰æ½œåœ¨çš„ãªå•é¡Œã‚’ç‰¹å®šï¼š
- æ–‡å­—ãŒä¸é®®æ˜Žãªç®‡æ‰€
- èƒŒæ™¯ã¨ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãŒä½Žã„éƒ¨åˆ†
- å°ã•ã™ãŽã‚‹ãƒ•ã‚©ãƒ³ãƒˆ
- æ‰‹æ›¸ãæ–‡å­—ã®æ··åœ¨
- ç‰¹æ®Šæ–‡å­—ãƒ»è¨˜å·ã®ä½¿ç”¨

## 4. å°‚é–€ç”¨èªžãƒ»å›ºæœ‰åè©žã®è­˜åˆ¥
- æŠ€è¡“ç”¨èªžã®ç¨®é¡žã¨åˆ†é‡Ž
- ä¼æ¥­åãƒ»è£½å“å
- äººåãƒ»åœ°å
- ç•¥èªžãƒ»è‹±æ•°å­—æ··åœ¨èªž

# å‡ºåŠ›å½¢å¼

**æ–‡æ›¸ã‚¿ã‚¤ãƒ—**: [è­˜åˆ¥ã•ã‚ŒãŸã‚¿ã‚¤ãƒ—]

**ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ§‹é€ **:
- è¡¨: [æœ‰ç„¡ã€æ§‹é€ è©³ç´°]
- ãƒªã‚¹ãƒˆ: [æœ‰ç„¡ã€éšŽå±¤æƒ…å ±]
- æ®µè½: [æ§‹é€ ã¨åŒºåˆ‡ã‚Š]
- ãã®ä»–: [ç‰¹å¾´çš„ãªè¦ç´ ]

**æ–‡å­—ãƒ»è¨€èªžæƒ…å ±**:
- ä¸»è¦è¨€èªž: [è¨€èªž]
- ãƒ•ã‚©ãƒ³ãƒˆç‰¹å¾´: [ã‚µã‚¤ã‚ºã€è£…é£¾ãªã©]
- æ–‡å­—å“è³ª: [é®®æ˜Žåº¦ã€ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆ]

**OCRæ³¨æ„äº‹é …**:
- èª²é¡Œã¨ãªã‚Šãã†ãªç®‡æ‰€
- å°‚é–€ç”¨èªžãƒ»å›ºæœ‰åè©žã®ãƒªã‚¹ãƒˆ
- æŽ¨å¥¨ã•ã‚Œã‚‹å‡¦ç†æ–¹é‡

**ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®æ¦‚è¦**:
- ä¸»é¡Œãƒ»ãƒˆãƒ”ãƒƒã‚¯
- é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
- æ–‡æ›¸ã®ç›®çš„ãƒ»æ„å›³

æ—¥æœ¬èªžã§è©³ç´°ã‹ã¤æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æžçµæžœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""

    def _fuse_and_correct(
        self,
        ocr_text: str,
        vision_analysis: Optional[Dict[str, Any]],
        image_path: str,
    ) -> Tuple[str, int, float, str]:
        """Fuse OCR and vision results with LLM correction."""
        if not self.enable_correction or not self.corrector:
            return ocr_text, 0, 0.0, "ocr_only"

        # Determine fusion strategy
        if vision_analysis:
            return self._fuse_with_vision(ocr_text, vision_analysis, image_path)
        else:
            return self._correct_ocr_only(ocr_text)

    def _fuse_with_vision(
        self, ocr_text: str, vision_analysis: Dict[str, Any], image_path: str
    ) -> Tuple[str, int, float, str]:
        """Fuse OCR and vision results using multimodal model for comprehensive analysis."""
        if not self.context_corrector:
            return self._correct_ocr_only(ocr_text)

        # Step 4: Multimodal fusion with comprehensive analysis
        try:
            # Use multimodal model for comprehensive text correction
            corrected_text, corrections_applied = self._multimodal_fusion_correction(
                ocr_text, vision_analysis, image_path
            )

            correction_ratio = corrections_applied / len(ocr_text) if ocr_text else 0.0
            return (
                corrected_text,
                corrections_applied,
                correction_ratio,
                "multimodal_fusion",
            )

        except Exception as e:
            logger.warning(
                f"Multimodal fusion failed, falling back to context-aware correction: {e}"
            )
            # Fallback to context-aware correction
            return self._context_aware_fusion_fallback(ocr_text, vision_analysis)

    def _correct_ocr_only(self, ocr_text: str) -> Tuple[str, int, float, str]:
        """Apply standard OCR correction."""
        if not self.corrector:
            return ocr_text, 0, 0.0, "ocr_only"

        corrected_text, corrections_applied = self.corrector.correct(ocr_text)
        correction_ratio = corrections_applied / len(ocr_text) if ocr_text else 0.0

        return corrected_text, corrections_applied, correction_ratio, "ocr_correction"

    def _multimodal_fusion_correction(
        self, ocr_text: str, vision_analysis: Dict[str, Any], image_path: str
    ) -> Tuple[str, int]:
        """
        Perform comprehensive multimodal fusion correction using vision model.

        This method combines:
        1. OCR extracted text
        2. Vision analysis results
        3. Original image
        4. Contextual metadata

        To produce the most accurate and coherent text output.
        """
        logger.debug("Starting multimodal fusion correction")

        try:
            # Encode image to base64 for multimodal input
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")

            # Create comprehensive multimodal prompt
            multimodal_prompt = self._create_multimodal_fusion_prompt(
                ocr_text, vision_analysis
            )

            # Prepare multimodal payload
            payload = {
                "model": self.vision_model,
                "prompt": multimodal_prompt,
                "images": [image_data],
                "stream": False,
                "options": {
                    "temperature": 0.05,  # Very low temperature for consistency
                    "top_p": 0.85,
                    "max_tokens": 2048,
                    "repeat_penalty": 1.1,
                },
            }

            logger.debug("Calling multimodal fusion model")
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=300,
            )

            if response.status_code == 200:
                result = response.json()
                corrected_text = result.get("response", "").strip()

                # Clean up the response
                corrected_text = self._clean_multimodal_response(
                    corrected_text, ocr_text
                )

                # Calculate corrections
                corrections_applied = self._calculate_corrections(
                    ocr_text, corrected_text
                )

                logger.info(
                    f"Multimodal fusion completed: {corrections_applied} corrections applied"
                )
                return corrected_text, corrections_applied
            else:
                logger.error(f"Multimodal fusion failed: {response.status_code}")
                raise Exception(f"API error: {response.status_code}")

        except Exception as e:
            logger.error(f"Multimodal fusion error: {e}")
            raise

    def _create_multimodal_fusion_prompt(
        self, ocr_text: str, vision_analysis: Dict[str, Any]
    ) -> str:
        """
        Create comprehensive multimodal fusion prompt following best practices.

        This prompt combines all available information to produce the most accurate result.
        """
        vision_analysis_text = vision_analysis.get("analysis", "")
        document_type = self._extract_document_type(vision_analysis)
        layout_info = self._extract_layout_info(vision_analysis)

        # Build context information
        context_info = []
        if document_type != "general":
            context_info.append(f"æ–‡æ›¸ã‚¿ã‚¤ãƒ—: {document_type}")

        if layout_info.get("has_tables"):
            context_info.append("è¡¨ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        if layout_info.get("has_lists"):
            context_info.append("ãƒªã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
        if layout_info.get("has_images"):
            context_info.append("ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã¾ã™")

        context_str = "ã€".join(context_info) if context_info else "ä¸€èˆ¬çš„ãªæ–‡æ›¸"

        return f"""ã‚ãªãŸã¯æœ€å…ˆç«¯ã®ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«OCRä¿®æ­£å°‚é–€å®¶ã§ã™ã€‚OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒåˆ†æžçµæžœã€åŽŸç”»åƒã®3ã¤ã®æƒ…å ±æºã‚’çµ±åˆã—ã€æœ€é«˜ç²¾åº¦ã®ãƒ†ã‚­ã‚¹ãƒˆä¿®æ­£ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

# ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æƒ…å ±çµ±åˆ

## ðŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

### ðŸ”¤ OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆï¼ˆä¸€æ¬¡ãƒ‡ãƒ¼ã‚¿ï¼‰
```
{ocr_text}
```

### ðŸ‘ï¸ ç”»åƒåˆ†æžçµæžœï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼‰
```
{vision_analysis_text}
```

### ðŸ“‹ æ–‡æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆæ§‹é€ æƒ…å ±ï¼‰
- **æ–‡æ›¸ã‚¿ã‚¤ãƒ—**: {context_str}
- **ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ**: {layout_info.get('layout_type', 'ãƒ•ãƒªãƒ¼ãƒ•ã‚©ãƒ¼ãƒ ')}
- **æ§‹é€ è¦ç´ **: 
  {'- è¡¨ã‚ã‚Š' if layout_info.get('has_tables') else ''}
  {'- ãƒªã‚¹ãƒˆã‚ã‚Š' if layout_info.get('has_lists') else ''}
  {'- ç”»åƒã‚ã‚Š' if layout_info.get('has_images') else ''}

# ðŸŽ¯ ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ä¿®æ­£æˆ¦ç•¥

## Phase 1: æƒ…å ±æºã®ä¿¡é ¼æ€§è©•ä¾¡
### OCRãƒ†ã‚­ã‚¹ãƒˆã®å“è³ªåˆ†æž:
- æ–‡å­—èªè­˜ç²¾åº¦ã®è©•ä¾¡
- æ§‹é€ çš„æ•´åˆæ€§ã®ç¢ºèª
- æ˜Žã‚‰ã‹ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š

### Visionåˆ†æžã¨ã®ç…§åˆ:
- ç”»åƒåˆ†æžçµæžœã¨OCRãƒ†ã‚­ã‚¹ãƒˆã®ä¸€è‡´åº¦
- æ–‡æ›¸ã‚¿ã‚¤ãƒ—ãƒ»ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã¨ã®æ•´åˆæ€§
- æ¬ è½æƒ…å ±ãƒ»è¿½åŠ æƒ…å ±ã®ç‰¹å®š

## Phase 2: çµ±åˆçš„ã‚¨ãƒ©ãƒ¼ä¿®æ­£
### 1. æ§‹é€ ãƒ¬ãƒ™ãƒ«ã®ä¿®æ­£
- **è¡¨æ§‹é€ **: ç”»åƒåˆ†æžã«åŸºã¥ãè¡Œãƒ»åˆ—ã®å¾©å…ƒ
- **ãƒªã‚¹ãƒˆæ§‹é€ **: éšŽå±¤ã¨ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã®æ­£ç¢ºãªå†ç¾
- **æ®µè½æ§‹é€ **: è«–ç†çš„ãªæ–‡æ›¸ãƒ•ãƒ­ãƒ¼ã®ç¶­æŒ

### 2. æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®é«˜ç²¾åº¦ä¿®æ­£
- **æ–‡è„ˆèªè­˜ä¿®æ­£**: ç”»åƒåˆ†æžçµæžœã‚’æ´»ç”¨ã—ãŸæ–‡å­—æŽ¨å®š
- **å°‚é–€ç”¨èªžä¿®æ­£**: æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãç”¨èªžã®æ­£ç¢ºæ€§ç¢ºä¿
- **è¨€èªžå“è³ª**: è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªžã¸ã®æœ€é©åŒ–

### 3. æ„å‘³ãƒ¬ãƒ™ãƒ«ã®æ•´åˆæ€§ç¢ºä¿
- **å†…å®¹ã®ä¸€è²«æ€§**: ç”»åƒå†…å®¹ã¨ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®å®Œå…¨ä¸€è‡´
- **æ–‡è„ˆã®é€£ç¶šæ€§**: æ–‡æ›¸å…¨ä½“ã®è«–ç†çš„ãªæµã‚Œã®ä¿æŒ
- **æƒ…å ±ã®å®Œå…¨æ€§**: æ¬ è½æƒ…å ±ã®è£œå®Œã¨å†—é•·æƒ…å ±ã®é™¤åŽ»

# ðŸ”§ å“è³ªä¿è¨¼åŸºæº–

## å¿…é ˆè¦ä»¶:
âœ… OCRèª¤èªè­˜ã®å®Œå…¨ä¿®æ­£
âœ… ç”»åƒåˆ†æžçµæžœã¨ã®100%æ•´åˆæ€§
âœ… å…ƒã®æƒ…å ±ãƒ»æ„å‘³ã®å®Œå…¨ä¿æŒ
âœ… æ–‡æ›¸æ§‹é€ ã®æ­£ç¢ºãªå†ç¾
âœ… è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªž

## ç¦æ­¢äº‹é …:
âŒ å…ƒã®æƒ…å ±ã®æ”¹å¤‰ãƒ»è¿½åŠ 
âŒ ç”»åƒåˆ†æžçµæžœã¨ã®çŸ›ç›¾
âŒ ä¸è‡ªç„¶ãªæ—¥æœ¬èªžè¡¨ç¾
âŒ æ§‹é€ æƒ…å ±ã®ç ´æ
âŒ èª¬æ˜Žãƒ»æ³¨é‡ˆã®è¿½åŠ 

# ðŸ“¤ æœ€çµ‚å‡ºåŠ›

**é‡è¦**: ä¿®æ­£ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜Žã€æ³¨é‡ˆã€ãƒ—ãƒ­ã‚»ã‚¹èª¬æ˜Žã¯ä¸€åˆ‡å«ã‚ãªã„ã§ãã ã•ã„ã€‚

**ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆä¿®æ­£çµæžœ**:"""

    def _clean_multimodal_response(self, response: str, original_text: str) -> str:
        """Clean up multimodal model response."""
        # Remove common prefixes (expanded list for better cleaning)
        prefixes_to_remove = [
            "ä¿®æ­£ç‰ˆãƒ†ã‚­ã‚¹ãƒˆ:",
            "ä¿®æ­£ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:",
            "ä¿®æ­£ç‰ˆ:",
            "ä¿®æ­£çµæžœ:",
            "å‡ºåŠ›:",
            "çµæžœ:",
            "ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆä¿®æ­£çµæžœ:",
            "æœ€çµ‚å‡ºåŠ›:",
            "ãƒ†ã‚­ã‚¹ãƒˆ:",
            "å›žç­”:",
            "ä¿®æ­£å¾Œ:",
            "ä¿®æ­£å†…å®¹:",
        ]

        cleaned = response.strip()
        for prefix in prefixes_to_remove:
            if cleaned.startswith(prefix):
                cleaned = cleaned[len(prefix) :].strip()

        # Remove any remaining prompt artifacts
        lines = cleaned.split("\n")
        filtered_lines = []
        for line in lines:
            # Skip lines that look like prompts or instructions
            if (
                not any(
                    keyword in line
                    for keyword in [
                        "ä¿®æ­£",
                        "åˆ†æž",
                        "æ–‡æ›¸",
                        "ç”»åƒ",
                        "OCR",
                        "ãƒ†ã‚­ã‚¹ãƒˆ",
                        "çµæžœ",
                        "å‡ºåŠ›",
                    ]
                )
                or len(line.strip()) > 10
            ):
                filtered_lines.append(line)

        result = "\n".join(filtered_lines).strip()

        # If result is too different from original, return original
        if len(result) < len(original_text) * 0.5:
            logger.warning("Multimodal response seems too short, using original text")
            return original_text

        return result

    def _calculate_corrections(self, original: str, corrected: str) -> int:
        """Calculate number of corrections made."""
        if not original or not corrected:
            return 0

        # Simple character-level difference calculation
        # This could be enhanced with more sophisticated diff algorithms
        original_chars = list(original)
        corrected_chars = list(corrected)

        # Use Levenshtein distance approximation
        max_len = max(len(original_chars), len(corrected_chars))
        min_len = min(len(original_chars), len(corrected_chars))

        # Count character differences
        differences = 0
        for i in range(min_len):
            if original_chars[i] != corrected_chars[i]:
                differences += 1

        # Add length difference
        differences += max_len - min_len

        return differences

    def _context_aware_fusion_fallback(
        self, ocr_text: str, vision_analysis: Dict[str, Any]
    ) -> Tuple[str, int, float, str]:
        """Fallback to context-aware correction when multimodal fusion fails."""
        logger.debug("Using context-aware correction fallback")

        # Create document metadata from vision analysis
        document_metadata = {
            "format": "image",
            "vision_analysis": vision_analysis.get("analysis", ""),
            "document_type": self._extract_document_type(vision_analysis),
            "layout_info": self._extract_layout_info(vision_analysis),
        }

        # Use context-aware correction with vision context
        corrected_text, corrections_applied = (
            self.context_corrector.correct_with_context(
                text=ocr_text,
                context_type="vision_enhanced",
                document_metadata=document_metadata,
            )
        )

        correction_ratio = corrections_applied / len(ocr_text) if ocr_text else 0.0
        return (
            corrected_text,
            corrections_applied,
            correction_ratio,
            "context_aware_fallback",
        )

    def _extract_document_type(self, vision_analysis: Dict[str, Any]) -> str:
        """Extract document type from vision analysis."""
        analysis = vision_analysis.get("analysis", "").lower()

        if "æŠ€è¡“" in analysis or "technical" in analysis:
            return "technical"
        elif "å­¦è¡“" in analysis or "è«–æ–‡" in analysis:
            return "academic"
        elif "ãƒ“ã‚¸ãƒã‚¹" in analysis or "business" in analysis:
            return "business"
        elif "è¡¨" in analysis or "table" in analysis:
            return "tabular"
        else:
            return "general"

    def _extract_layout_info(self, vision_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Extract layout information from vision analysis."""
        analysis = vision_analysis.get("analysis", "")

        return {
            "has_tables": "è¡¨" in analysis or "table" in analysis,
            "has_images": "ç”»åƒ" in analysis or "image" in analysis,
            "has_lists": "ãƒªã‚¹ãƒˆ" in analysis or "list" in analysis,
            "layout_type": "structured" if "æ§‹é€ " in analysis else "freeform",
        }

    def _calculate_confidence(self, ocr_data: Dict[str, Any]) -> float:
        """Calculate average confidence score from Tesseract output."""
        confidences = [int(conf) for conf in ocr_data["conf"] if int(conf) > 0]
        return sum(confidences) / len(confidences) / 100.0 if confidences else 0.0

    def get_supported_vision_models(self) -> List[str]:
        """Get list of supported vision models."""
        return ["llava", "llava:7b", "llava:13b", "bakllava"]

    def check_vision_model_availability(self) -> bool:
        """Check if vision model is available."""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                return self.vision_model in available_models
            return False
        except Exception:
            return False
